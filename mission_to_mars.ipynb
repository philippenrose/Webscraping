{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":"import pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup as bs\nfrom splinter import Browser\nimport time"},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":"#tired of calling it for things.\ndef CallBrowser():\n    executable_path = {'executable_path': 'C:/Users/User/Documents/Homework/Assingment_WebScrapping/chromedriver.exe'}\n    browser = Browser('chrome', **executable_path, headless=False) \n    return browser \n\n#using this to make sure i was actually looking at the correct thing in here.\ndef OutputSoup(soup):\n    with open('x.html','w',encoding='utf-8') as file:\n        file.write(str(soup))"},{"cell_type":"markdown","metadata":{"collapsed":true},"outputs":[],"source":"# Scrape everything\n"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":"# this dictionary will hold everything we pull from all the sites\nscraped_data = {}"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":"# site 1 -https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest\nnews_url=\"https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest\"\ntempbrowser=CallBrowser()\ntempbrowser.visit(news_url)\n\n#had to use a timer otherwise the javascript wouldnt be seen. that one took a minute to realize.\ntime.sleep(5)\n# Parse HTML with Beautiful Soup\nhtml = tempbrowser.html\nsoup = bs(html,'html.parser')\ntempbrowser.quit()\n\n# use bs to find() the example_title_div and filter on the class_='content_tile'\nlevel1=soup.find_all('div', class_='content_title')\nnews_title = level1[0].text.strip()\nscraped_data['news_title'] = news_title\n\n# use bs to find() the example_title_div and filter on the class_='article_teaser_body'\nlevel1=soup.find_all('div',class_='article_teaser_body')\nnews_p=level1[0].text.strip()\nscraped_data['news_p'] = news_p\n"},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":"# site 2 - https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\nImage_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n# use splinter to connect to the url and navigate, then use bs4 to repeat what you did in site 1\ntempbrowser=CallBrowser()\ntempbrowser.visit(Image_url)\n#super navigation fun time.\ntime.sleep(5)\ntempbrowser.click_link_by_partial_text('FULL IMAGE')\ntime.sleep(5)\ntempbrowser.click_link_by_partial_text('more info')\ntime.sleep(5)\n\nhtml = tempbrowser.html\nsoup = bs(html,'html.parser')\ntempbrowser.quit()\n\ndump = soup.find('figure', class_='lede').find('a')['href']\nfeatured_image_url = 'https://www.jpl.nasa.gov' + dump\nscraped_data['featured_image_url'] = featured_image_url\n"},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":"# site 3 - https://twitter.com/marswxreport?lang=en\nweather_url = 'https://twitter.com/marswxreport?lang=en'\ntempbrowser=CallBrowser()\ntempbrowser.visit(weather_url)\nhtml = tempbrowser.html\nsoup = bs(html,'html.parser')\ntempbrowser.quit()\n\n# grab the latest tweet and be careful its a weather tweet\ntw=soup.find_all('p',class_='TweetTextSize TweetTextSize--normal js-tweet-text tweet-text')\nmars_weather=tw[0].text.strip()\nscraped_data['mars_weather'] = mars_weather\n"},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":"# site 4 - https://space-facts.com/mars/\nfacts_url = 'https://space-facts.com/mars/'\ntempbrowser=CallBrowser()\ntempbrowser.visit(facts_url)\ntime.sleep(5)\nhtml = tempbrowser.html\nsoup = bs(html,'html.parser')\ntempbrowser.quit()\n#OutputSoup(soup)\n# use pandas to parse the table\ntables = pd.read_html(facts_url)[0]\ntables = tables.rename(columns={'Mars - Earth Comparison':'Descriptions','Mars':'Values'})\nfacts_df=tables[['Descriptions','Values']]\nfacts_df.set_index('Descriptions', inplace=True)\n# convert facts_df to a html string and add to dictionary.\nfacts_html=facts_df.to_html()\nscraped_data['facts_html'] = facts_html\n"},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":"# site 5 - https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\nhemi_url='https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\nhemi_url_backup='http://www.planetary.org/blogs/guest-blogs/bill-dunford/20140203-the-faces-of-mars.html'\ntempbrowser=CallBrowser()\ntempbrowser.visit(hemi_url_backup)\nhtml = tempbrowser.html\nsoup = bs(html,'html.parser')\ntempbrowser.quit()\n# OutputSoup(soup)\n# use bs4 to scrape the title and url and add to dictionary\nitr = range(1,5)\nhemisphere_image_urls = []\n\nfor n in itr:\n    thisdict={}\n    lp=soup.find_all('div', class_='img-caption-box')[n]\n    thisdict['title']=lp.find('h5').text.strip()\n    thisdict['img_url']=lp.find('a')['href']\n    hemisphere_image_urls.append(thisdict)\n\n\nscraped_data['hemisphere_image_urls'] = hemisphere_image_urls\n    "},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# File-> download as python into a new module called scrape_mars.py"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# use day 3 09-Ins_Scrape_And_Render/app.py as a blue print on how to finish the homework.\n\n# replace the contents of def index() and def scraper() appropriately.\n\n# change the index.html to render the site with all the data."}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}